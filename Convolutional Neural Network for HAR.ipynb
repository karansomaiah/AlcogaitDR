{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "with open('UCI HAR Dataset/train/X_train.txt') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        temp = row[0].split(\" \")\n",
    "        temp_list = []\n",
    "        for x in temp:\n",
    "            if x != '':\n",
    "                temp_list.append(float(x))\n",
    "        X_train.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "with open('UCI HAR Dataset/test/X_test.txt') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        temp = row[0].split(\" \")\n",
    "        temp_list = []\n",
    "        for x in temp:\n",
    "            if x != '':\n",
    "                temp_list.append(float(x))\n",
    "        X_test.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = []\n",
    "with open('UCI HAR Dataset/train/y_train.txt') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        y_train.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = []\n",
    "with open('UCI HAR Dataset/test/y_test.txt') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        y_test.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_ = [(x-1) for x in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_ = [(x-1) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train_)\n",
    "y_test = to_categorical(y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274488</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.109141</td>\n",
       "      <td>-0.605438</td>\n",
       "      <td>-0.510938</td>\n",
       "      <td>-0.604754</td>\n",
       "      <td>-0.630512</td>\n",
       "      <td>-0.526907</td>\n",
       "      <td>-0.606150</td>\n",
       "      <td>-0.468604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125293</td>\n",
       "      <td>-0.307009</td>\n",
       "      <td>-0.625294</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.489547</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>-0.056515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.448734</td>\n",
       "      <td>0.502645</td>\n",
       "      <td>0.418687</td>\n",
       "      <td>0.424073</td>\n",
       "      <td>0.485942</td>\n",
       "      <td>0.414122</td>\n",
       "      <td>0.544547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.321011</td>\n",
       "      <td>0.307584</td>\n",
       "      <td>0.336787</td>\n",
       "      <td>0.448306</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.477975</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.297480</td>\n",
       "      <td>0.279122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995357</td>\n",
       "      <td>-0.999765</td>\n",
       "      <td>-0.976580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262975</td>\n",
       "      <td>-0.024863</td>\n",
       "      <td>-0.120993</td>\n",
       "      <td>-0.992754</td>\n",
       "      <td>-0.978129</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>-0.993591</td>\n",
       "      <td>-0.978162</td>\n",
       "      <td>-0.980251</td>\n",
       "      <td>-0.936219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023692</td>\n",
       "      <td>-0.542602</td>\n",
       "      <td>-0.845573</td>\n",
       "      <td>-0.121527</td>\n",
       "      <td>-0.289549</td>\n",
       "      <td>-0.482273</td>\n",
       "      <td>-0.376341</td>\n",
       "      <td>-0.812065</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.143414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277193</td>\n",
       "      <td>-0.017219</td>\n",
       "      <td>-0.108676</td>\n",
       "      <td>-0.946196</td>\n",
       "      <td>-0.851897</td>\n",
       "      <td>-0.859365</td>\n",
       "      <td>-0.950709</td>\n",
       "      <td>-0.857328</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.881637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>-0.343685</td>\n",
       "      <td>-0.711692</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.709417</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288461</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.097794</td>\n",
       "      <td>-0.242813</td>\n",
       "      <td>-0.034231</td>\n",
       "      <td>-0.262415</td>\n",
       "      <td>-0.292680</td>\n",
       "      <td>-0.066701</td>\n",
       "      <td>-0.265671</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289096</td>\n",
       "      <td>-0.126979</td>\n",
       "      <td>-0.503878</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.292861</td>\n",
       "      <td>0.506187</td>\n",
       "      <td>0.359368</td>\n",
       "      <td>-0.509079</td>\n",
       "      <td>0.248353</td>\n",
       "      <td>0.107659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.274488    -0.017695    -0.109141    -0.605438    -0.510938   \n",
       "std       0.070261     0.040811     0.056635     0.448734     0.502645   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -0.999873   \n",
       "25%       0.262975    -0.024863    -0.120993    -0.992754    -0.978129   \n",
       "50%       0.277193    -0.017219    -0.108676    -0.946196    -0.851897   \n",
       "75%       0.288461    -0.010783    -0.097794    -0.242813    -0.034231   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.916238   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean     -0.604754    -0.630512    -0.526907    -0.606150    -0.468604   \n",
       "std       0.418687     0.424073     0.485942     0.414122     0.544547   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.980233    -0.993591    -0.978162    -0.980251    -0.936219   \n",
       "50%      -0.859365    -0.950709    -0.857328    -0.857143    -0.881637   \n",
       "75%      -0.262415    -0.292680    -0.066701    -0.265671    -0.017129   \n",
       "max       1.000000     1.000000     0.967664     1.000000     1.000000   \n",
       "\n",
       "          ...               551          552          553          554  \\\n",
       "count     ...       7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      ...          0.125293    -0.307009    -0.625294     0.008684   \n",
       "std       ...          0.250994     0.321011     0.307584     0.336787   \n",
       "min       ...         -1.000000    -0.995357    -0.999765    -0.976580   \n",
       "25%       ...         -0.023692    -0.542602    -0.845573    -0.121527   \n",
       "50%       ...          0.134000    -0.343685    -0.711692     0.009509   \n",
       "75%       ...          0.289096    -0.126979    -0.503878     0.150865   \n",
       "max       ...          0.946700     0.989538     0.956845     1.000000   \n",
       "\n",
       "               555          556          557          558          559  \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.002186     0.008726    -0.005981    -0.489547     0.058593   \n",
       "std       0.448306     0.608303     0.477975     0.511807     0.297480   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.289549    -0.482273    -0.376341    -0.812065    -0.017885   \n",
       "50%       0.008943     0.008735    -0.000368    -0.709417     0.182071   \n",
       "75%       0.292861     0.506187     0.359368    -0.509079     0.248353   \n",
       "max       1.000000     0.998702     0.996078     1.000000     0.478157   \n",
       "\n",
       "               560  \n",
       "count  7352.000000  \n",
       "mean     -0.056515  \n",
       "std       0.279122  \n",
       "min      -1.000000  \n",
       "25%      -0.143414  \n",
       "50%       0.003181  \n",
       "75%       0.107659  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.273996</td>\n",
       "      <td>-0.017863</td>\n",
       "      <td>-0.108386</td>\n",
       "      <td>-0.613635</td>\n",
       "      <td>-0.508330</td>\n",
       "      <td>-0.633797</td>\n",
       "      <td>-0.641278</td>\n",
       "      <td>-0.522676</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>-0.462063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130236</td>\n",
       "      <td>-0.277593</td>\n",
       "      <td>-0.598756</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.040029</td>\n",
       "      <td>-0.017298</td>\n",
       "      <td>-0.513923</td>\n",
       "      <td>0.074886</td>\n",
       "      <td>-0.048720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.060570</td>\n",
       "      <td>0.025745</td>\n",
       "      <td>0.042747</td>\n",
       "      <td>0.412597</td>\n",
       "      <td>0.494269</td>\n",
       "      <td>0.362699</td>\n",
       "      <td>0.385199</td>\n",
       "      <td>0.479899</td>\n",
       "      <td>0.357753</td>\n",
       "      <td>0.523916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231018</td>\n",
       "      <td>0.317245</td>\n",
       "      <td>0.311042</td>\n",
       "      <td>0.336147</td>\n",
       "      <td>0.445077</td>\n",
       "      <td>0.634989</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>0.509205</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.241467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.592004</td>\n",
       "      <td>-0.362884</td>\n",
       "      <td>-0.576184</td>\n",
       "      <td>-0.999606</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.998955</td>\n",
       "      <td>-0.999417</td>\n",
       "      <td>-0.999914</td>\n",
       "      <td>-0.998899</td>\n",
       "      <td>-0.952357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.785543</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.993402</td>\n",
       "      <td>-0.998898</td>\n",
       "      <td>-0.991096</td>\n",
       "      <td>-0.984195</td>\n",
       "      <td>-0.913704</td>\n",
       "      <td>-0.949228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262075</td>\n",
       "      <td>-0.024961</td>\n",
       "      <td>-0.121162</td>\n",
       "      <td>-0.990914</td>\n",
       "      <td>-0.973664</td>\n",
       "      <td>-0.976122</td>\n",
       "      <td>-0.992333</td>\n",
       "      <td>-0.974131</td>\n",
       "      <td>-0.975352</td>\n",
       "      <td>-0.934447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008433</td>\n",
       "      <td>-0.517494</td>\n",
       "      <td>-0.829593</td>\n",
       "      <td>-0.130541</td>\n",
       "      <td>-0.282600</td>\n",
       "      <td>-0.518924</td>\n",
       "      <td>-0.428375</td>\n",
       "      <td>-0.829722</td>\n",
       "      <td>0.022140</td>\n",
       "      <td>-0.098485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277113</td>\n",
       "      <td>-0.016967</td>\n",
       "      <td>-0.108458</td>\n",
       "      <td>-0.931214</td>\n",
       "      <td>-0.790972</td>\n",
       "      <td>-0.827534</td>\n",
       "      <td>-0.937664</td>\n",
       "      <td>-0.799907</td>\n",
       "      <td>-0.817005</td>\n",
       "      <td>-0.852659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142676</td>\n",
       "      <td>-0.311023</td>\n",
       "      <td>-0.683672</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.047113</td>\n",
       "      <td>-0.026726</td>\n",
       "      <td>-0.729648</td>\n",
       "      <td>0.181563</td>\n",
       "      <td>-0.010671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288097</td>\n",
       "      <td>-0.010143</td>\n",
       "      <td>-0.097123</td>\n",
       "      <td>-0.267395</td>\n",
       "      <td>-0.105919</td>\n",
       "      <td>-0.311432</td>\n",
       "      <td>-0.321719</td>\n",
       "      <td>-0.133488</td>\n",
       "      <td>-0.322771</td>\n",
       "      <td>-0.009965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288320</td>\n",
       "      <td>-0.083559</td>\n",
       "      <td>-0.458332</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.288113</td>\n",
       "      <td>0.622151</td>\n",
       "      <td>0.394387</td>\n",
       "      <td>-0.545939</td>\n",
       "      <td>0.260252</td>\n",
       "      <td>0.092373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.671887</td>\n",
       "      <td>0.246106</td>\n",
       "      <td>0.494114</td>\n",
       "      <td>0.465299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.489703</td>\n",
       "      <td>0.439657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.427958</td>\n",
       "      <td>0.786436</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998898</td>\n",
       "      <td>0.986347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  2947.000000  2947.000000  2947.000000  2947.000000  2947.000000   \n",
       "mean      0.273996    -0.017863    -0.108386    -0.613635    -0.508330   \n",
       "std       0.060570     0.025745     0.042747     0.412597     0.494269   \n",
       "min      -0.592004    -0.362884    -0.576184    -0.999606    -1.000000   \n",
       "25%       0.262075    -0.024961    -0.121162    -0.990914    -0.973664   \n",
       "50%       0.277113    -0.016967    -0.108458    -0.931214    -0.790972   \n",
       "75%       0.288097    -0.010143    -0.097123    -0.267395    -0.105919   \n",
       "max       0.671887     0.246106     0.494114     0.465299     1.000000   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  2947.000000  2947.000000  2947.000000  2947.000000  2947.000000   \n",
       "mean     -0.633797    -0.641278    -0.522676    -0.637038    -0.462063   \n",
       "std       0.362699     0.385199     0.479899     0.357753     0.523916   \n",
       "min      -0.998955    -0.999417    -0.999914    -0.998899    -0.952357   \n",
       "25%      -0.976122    -0.992333    -0.974131    -0.975352    -0.934447   \n",
       "50%      -0.827534    -0.937664    -0.799907    -0.817005    -0.852659   \n",
       "75%      -0.311432    -0.321719    -0.133488    -0.322771    -0.009965   \n",
       "max       0.489703     0.439657     1.000000     0.427958     0.786436   \n",
       "\n",
       "          ...               551          552          553          554  \\\n",
       "count     ...       2947.000000  2947.000000  2947.000000  2947.000000   \n",
       "mean      ...          0.130236    -0.277593    -0.598756     0.005264   \n",
       "std       ...          0.231018     0.317245     0.311042     0.336147   \n",
       "min       ...         -0.785543    -1.000000    -1.000000    -1.000000   \n",
       "25%       ...         -0.008433    -0.517494    -0.829593    -0.130541   \n",
       "50%       ...          0.142676    -0.311023    -0.683672     0.005188   \n",
       "75%       ...          0.288320    -0.083559    -0.458332     0.146200   \n",
       "max       ...          1.000000     1.000000     1.000000     0.998898   \n",
       "\n",
       "               555          556          557          558          559  \\\n",
       "count  2947.000000  2947.000000  2947.000000  2947.000000  2947.000000   \n",
       "mean      0.003799     0.040029    -0.017298    -0.513923     0.074886   \n",
       "std       0.445077     0.634989     0.501311     0.509205     0.324300   \n",
       "min      -0.993402    -0.998898    -0.991096    -0.984195    -0.913704   \n",
       "25%      -0.282600    -0.518924    -0.428375    -0.829722     0.022140   \n",
       "50%       0.006767     0.047113    -0.026726    -0.729648     0.181563   \n",
       "75%       0.288113     0.622151     0.394387    -0.545939     0.260252   \n",
       "max       0.986347     1.000000     1.000000     0.833180     1.000000   \n",
       "\n",
       "               560  \n",
       "count  2947.000000  \n",
       "mean     -0.048720  \n",
       "std       0.241467  \n",
       "min      -0.949228  \n",
       "25%      -0.098485  \n",
       "50%      -0.010671  \n",
       "75%       0.092373  \n",
       "max       0.973113  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_reshaped = np.array(X_train).reshape(7352,561,1)\n",
    "X_test_reshaped = np.array(X_test).reshape(2947,561,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data set has been imported. A bit of information about the dataset.\n",
    "- The labels have 6 activities: (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING)\n",
    "- Train has 7352 rows and 561 columns\n",
    "- Test has 2947 rows and 561 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building the 1D Convolutional Neural Network with the Dataset according the architecture provided by the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(120, 11, activation='relu', input_shape=(561,1)))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(130, 11, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(128, 11, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 551, 120)          1440      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 275, 120)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 265, 130)          171730    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 132, 130)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 122, 128)          183168    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 61, 128)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 357,112\n",
      "Trainable params: 357,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 7352 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.4503 - acc: 0.8333 - val_loss: 0.4494 - val_acc: 0.8333\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.4486 - acc: 0.8333 - val_loss: 0.4478 - val_acc: 0.8333\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.4472 - acc: 0.8333 - val_loss: 0.4466 - val_acc: 0.8333\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.4461 - acc: 0.8333 - val_loss: 0.4455 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.4451 - acc: 0.8333 - val_loss: 0.4446 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.4441 - acc: 0.8333 - val_loss: 0.4437 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.4432 - acc: 0.8333 - val_loss: 0.4427 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.4423 - acc: 0.8333 - val_loss: 0.4418 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 136s 18ms/step - loss: 0.4414 - acc: 0.8333 - val_loss: 0.4409 - val_acc: 0.8333\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.4404 - acc: 0.8333 - val_loss: 0.4399 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.4395 - acc: 0.8333 - val_loss: 0.4390 - val_acc: 0.8333\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.4385 - acc: 0.8333 - val_loss: 0.4379 - val_acc: 0.8333\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.4374 - acc: 0.8333 - val_loss: 0.4368 - val_acc: 0.8333\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.4363 - acc: 0.8333 - val_loss: 0.4356 - val_acc: 0.8333\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.4350 - acc: 0.8333 - val_loss: 0.4344 - val_acc: 0.8333\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.4338 - acc: 0.8333 - val_loss: 0.4330 - val_acc: 0.8333\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.4324 - acc: 0.8333 - val_loss: 0.4316 - val_acc: 0.8333\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.4309 - acc: 0.8333 - val_loss: 0.4300 - val_acc: 0.8333\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.4293 - acc: 0.8333 - val_loss: 0.4284 - val_acc: 0.8333\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.4276 - acc: 0.8333 - val_loss: 0.4266 - val_acc: 0.8333\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.4257 - acc: 0.8333 - val_loss: 0.4247 - val_acc: 0.8333\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.4237 - acc: 0.8333 - val_loss: 0.4226 - val_acc: 0.8333\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.4215 - acc: 0.8333 - val_loss: 0.4202 - val_acc: 0.8333\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.4191 - acc: 0.8333 - val_loss: 0.4178 - val_acc: 0.8333\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.4165 - acc: 0.8333 - val_loss: 0.4150 - val_acc: 0.8333\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.4136 - acc: 0.8333 - val_loss: 0.4120 - val_acc: 0.8333\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.4105 - acc: 0.8333 - val_loss: 0.4088 - val_acc: 0.8333\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.4072 - acc: 0.8333 - val_loss: 0.4053 - val_acc: 0.8333\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.4035 - acc: 0.8333 - val_loss: 0.4015 - val_acc: 0.8333\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.3995 - acc: 0.8333 - val_loss: 0.3974 - val_acc: 0.8333\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.3953 - acc: 0.8333 - val_loss: 0.3928 - val_acc: 0.8333\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.3906 - acc: 0.8333 - val_loss: 0.3880 - val_acc: 0.8333\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.3857 - acc: 0.8333 - val_loss: 0.3830 - val_acc: 0.8333\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.3805 - acc: 0.8334 - val_loss: 0.3777 - val_acc: 0.8334\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.3752 - acc: 0.8334 - val_loss: 0.3723 - val_acc: 0.8334\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.3698 - acc: 0.8335 - val_loss: 0.3670 - val_acc: 0.8337\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.3646 - acc: 0.8338 - val_loss: 0.3619 - val_acc: 0.8339\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 131s 18ms/step - loss: 0.3596 - acc: 0.8340 - val_loss: 0.3569 - val_acc: 0.8341\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.3547 - acc: 0.8342 - val_loss: 0.3522 - val_acc: 0.8343\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.3502 - acc: 0.8345 - val_loss: 0.3481 - val_acc: 0.8346\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.3460 - acc: 0.8348 - val_loss: 0.3441 - val_acc: 0.8349\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.3423 - acc: 0.8350 - val_loss: 0.3403 - val_acc: 0.8351\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.3388 - acc: 0.8352 - val_loss: 0.3370 - val_acc: 0.8356\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.3356 - acc: 0.8355 - val_loss: 0.3340 - val_acc: 0.8358\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3328 - acc: 0.8358 - val_loss: 0.3313 - val_acc: 0.8358\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 1414s 192ms/step - loss: 0.3304 - acc: 0.8358 - val_loss: 0.3289 - val_acc: 0.8358\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.3280 - acc: 0.8360 - val_loss: 0.3269 - val_acc: 0.8359\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.3260 - acc: 0.8360 - val_loss: 0.3250 - val_acc: 0.8359\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.3243 - acc: 0.8359 - val_loss: 0.3232 - val_acc: 0.8363\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.3226 - acc: 0.8361 - val_loss: 0.3219 - val_acc: 0.8363\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.3213 - acc: 0.8363 - val_loss: 0.3206 - val_acc: 0.8363\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.3199 - acc: 0.8364 - val_loss: 0.3191 - val_acc: 0.8362\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.3188 - acc: 0.8360 - val_loss: 0.3180 - val_acc: 0.8361\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.3177 - acc: 0.8360 - val_loss: 0.3171 - val_acc: 0.8360\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 117s 16ms/step - loss: 0.3167 - acc: 0.8360 - val_loss: 0.3160 - val_acc: 0.8360\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.3159 - acc: 0.8362 - val_loss: 0.3153 - val_acc: 0.8361\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3153 - acc: 0.8363 - val_loss: 0.3144 - val_acc: 0.8362\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.3143 - acc: 0.8361 - val_loss: 0.3139 - val_acc: 0.8362\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.3137 - acc: 0.8360 - val_loss: 0.3138 - val_acc: 0.8360\n",
      "Epoch 60/100\n",
      "7352/7352 [==============================] - 125s 17ms/step - loss: 0.3132 - acc: 0.8363 - val_loss: 0.3128 - val_acc: 0.8361\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.3127 - acc: 0.8362 - val_loss: 0.3121 - val_acc: 0.8362\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.3121 - acc: 0.8361 - val_loss: 0.3117 - val_acc: 0.8362\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 118s 16ms/step - loss: 0.3116 - acc: 0.8362 - val_loss: 0.3114 - val_acc: 0.8362\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3114 - acc: 0.8361 - val_loss: 0.3107 - val_acc: 0.8362\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.3108 - acc: 0.8363 - val_loss: 0.3106 - val_acc: 0.8362\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.3107 - acc: 0.8361 - val_loss: 0.3105 - val_acc: 0.8361\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.3102 - acc: 0.8362 - val_loss: 0.3101 - val_acc: 0.8362\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.3100 - acc: 0.8361 - val_loss: 0.3117 - val_acc: 0.8361\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3098 - acc: 0.8359 - val_loss: 0.3091 - val_acc: 0.8362\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3093 - acc: 0.8364 - val_loss: 0.3088 - val_acc: 0.8362\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3091 - acc: 0.8361 - val_loss: 0.3091 - val_acc: 0.8368\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.3091 - acc: 0.8361 - val_loss: 0.3084 - val_acc: 0.8361\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.3087 - acc: 0.8362 - val_loss: 0.3083 - val_acc: 0.8363\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3086 - acc: 0.8361 - val_loss: 0.3083 - val_acc: 0.8362\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3084 - acc: 0.8365 - val_loss: 0.3080 - val_acc: 0.8361\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.3081 - acc: 0.8360 - val_loss: 0.3089 - val_acc: 0.8365\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.3080 - acc: 0.8367 - val_loss: 0.3075 - val_acc: 0.8361\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.3077 - acc: 0.8365 - val_loss: 0.3075 - val_acc: 0.8364\n",
      "Epoch 79/100\n",
      " 768/7352 [==>...........................] - ETA: 1:23 - loss: 0.3064 - acc: 0.8372"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_reshaped, y_train, batch_size=128, validation_data=(X_train_reshaped,y_train), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 12s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_reshaped, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
